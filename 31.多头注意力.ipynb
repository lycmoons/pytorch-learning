{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eacc3bd4",
   "metadata": {},
   "source": [
    "### 多头注意力机制\n",
    "\n",
    "对一组 Q、K、V，我们将三个矩阵映射到同一个向量空间（hiden_size）\n",
    "\n",
    "然后对 Q、K 进行相关性学习时，考虑将两个向量按照维度分为 n 个部分分别进行点积学习\n",
    "\n",
    "得到 V 向量的各个维度的权重\n",
    "\n",
    "这就是 n 头注意力，图解如下：\n",
    "\n",
    "![](md-img\\多头注意力权重.jpg)\n",
    "\n",
    "然后使用多头注意力权重，计算最终的输出：\n",
    "\n",
    "![](md-img\\多头注意力输出.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875a4cf",
   "metadata": {},
   "source": [
    "`其实多头注意力就可以理解为将 Q、K、V 最后一个维度分为多个部分，每个部分分别应用单头注意力机制`\n",
    "\n",
    "这样做的效果是可以发掘序列中的每一个 token 的不同的表征 `子空间` 之间的相关性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644dd6c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 代码实现\n",
    "\n",
    "虽然多头注意力是在不同的维度实行单头注意力，但各个维度的单头注意力是可以并行的进行的（一次计算完成）\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a3bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0173e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_q = nn.Linear(query_size, hidden_size * num_heads)\n",
    "        self.linear_k = nn.Linear(key_size, hidden_size * num_heads)\n",
    "        self.linear_v = nn.Linear(value_size, hidden_size * num_heads)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.linear_o = nn.Linear(hidden_size * num_heads, hidden_size * num_heads)\n",
    "\n",
    "    # query 的形状：(batch_size, num_querys, query_size)\n",
    "    # key 的形状：(batch_size, num_pairs, key_size)\n",
    "    # value 的形状：(batch_size, num_pairs, value_size)\n",
    "    def forward(self, query, key, value):\n",
    "        batch_size = query.shape[0]\n",
    "        num_querys = query.shape[1]\n",
    "        num_pairs = key.shape[1]\n",
    "\n",
    "        query = self.linear_q(query)    # (batch_size, num_querys, hidden_size * num_heads)\n",
    "        key = self.linear_k(key)        # (batch_size, num_pairs, hidden_size * num_heads)\n",
    "        value = self.linear_v(value)    # (batch_size, num_pairs, hidden_size * num_heads)\n",
    "\n",
    "        # 分头\n",
    "        query = query.reshape(batch_size, num_querys, self.num_heads, -1)    # (batch_size, num_querys, num_heads, hidden_size)\n",
    "        key = key.reshape(batch_size, num_pairs, self.num_heads, -1)         # (batch_size, num_pairs, num_heads, hidden_size)\n",
    "        value = value.reshape(batch_size, num_pairs, self.num_heads, -1)     # (batch_size, num_pairs, num_heads, hidden_size)\n",
    "\n",
    "        # 分别对各个头计算点积注意力权重\n",
    "        query = query.permute(0, 2, 1, 3).reshape(batch_size * self.num_heads, num_querys, -1)     # (batch_size * num_heads, num_querys, hidden_size)\n",
    "        key = key.permute(0, 2, 3, 1).reshape(batch_size * self.num_heads, self.hidden_size, -1)  # (batch_size * num_heads, hidden_size, num_pairs)\n",
    "        score = torch.bmm(query, key) / math.sqrt(self.hidden_size)    # (batch_size * num_heads, num_querys, num_pairs)\n",
    "        weight = self.softmax(score)    # (batch_size * num_heads, num_querys, num_pairs)\n",
    "\n",
    "        # 分别计算每一个头的输出值\n",
    "        value = value.permute(0, 2, 1, 3).reshape(batch_size * self.num_heads, num_pairs, -1)  # (batch_size * num_heads, num_pairs, hidden_size)\n",
    "        output = torch.bmm(weight, value)    # (batch_size * num_heads, num_querys, hidden_size)\n",
    "\n",
    "        # 拼接每一个头\n",
    "        output =  output.reshape(batch_size, self.num_heads, num_querys, -1)       # (batch_size, num_heads, num_querys, hidden_size)\n",
    "        output = output.permute(0, 2, 1, 3).reshape(batch_size, num_querys, -1)    # (batch_size, num_querys, num_heads * hidden_size)\n",
    "\n",
    "        # 进行最后的线性融合\n",
    "        return self.linear_o(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
