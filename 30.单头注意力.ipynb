{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0ce4ede",
   "metadata": {},
   "source": [
    "### 单头注意力\n",
    "\n",
    "单头注意力一般使用点积注意力，Scaled Dot-Product Attention\n",
    "\n",
    "即使用两个向量之间的点积来衡量两个向量 (query 和 key) 之间的相关性\n",
    "\n",
    "首先对查询向量以及键值对向量进行线性映射，让三个向量都映射到同一个向量空间中（维度相同）\n",
    "\n",
    "然后再做点积获取注意力分数，再通过 softmax 层进行归一化得到注意力权重，最后应用到各个值得到最终的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ecbddc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "$$\n",
    "Q = QW^Q, \\quad K = KW^K, \\quad V = VW^V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Attention}(X) = \n",
    "\\operatorname{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "$$\n",
    "其中 d_k 表示进行线性映射后 Q、K、V 的向量维度\n",
    "$$\n",
    "\n",
    "$$\n",
    "除\\sqrt{d_k}的作用是防止向量点积之间的方差过大，导致softmax饱和\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c082e4",
   "metadata": {},
   "source": [
    "单头注意力图解如下：\n",
    "\n",
    "![](md-img\\单头注意力.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb73088d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2759a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d9aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, model_size):\n",
    "        super().__init__()\n",
    "        self.model_size = torch.tensor(model_size)\n",
    "        self.linear_q = nn.Linear(query_size, model_size)\n",
    "        self.linear_k = nn.Linear(key_size, model_size)\n",
    "        self.linear_v = nn.Linear(value_size, model_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    # query：(batch_size, num_querys, query_size)\n",
    "    # key：(batch_size, num_pairs, key_size)\n",
    "    # value：(batch_size, num_pairs, value_size)\n",
    "    def forward(self, query, key, value):\n",
    "        query = self.linear_q(query)    # (batch_size, num_querys, model_size)\n",
    "        key = self.linear_k(key)        # (batch_size, num_pairs, model_size)\n",
    "        value = self.linear_v(value)    # (batch_size, num_pairs, model_size)\n",
    "        score = torch.bmm(query, key.permute(0, 2, 1))    # (batch_size, num_querys, num_pairs)\n",
    "        score = score / torch.sqrt(self.model_size)       # (batch_size, num_querys, num_pairs)，scale\n",
    "        weight = self.softmax(score)    # (batch_size, num_querys, num_pairs)\n",
    "        return torch.bmm(weight, value) # (batch_size, num_querys, model_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
