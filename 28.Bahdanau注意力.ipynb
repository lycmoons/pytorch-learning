{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2a51c9",
   "metadata": {},
   "source": [
    "### Bahdanau 注意力\n",
    "\n",
    "Bahdanau 注意力就是将注意力机制应用于 seq to seq 模型上（机器翻译）\n",
    "\n",
    "主要的想法是，在进行翻译时，我们翻译一个词可能只对应原文中的每一个小部分\n",
    "\n",
    "在原本机器翻译模型上，解码器的隐藏状态初始值仍然使用编码器最后一层的隐藏状态来赋值\n",
    "\n",
    "但是 context 不再使用最后一个时间步、最后一层隐藏状态，而是考虑最后一层每一个时间步的隐藏状态，权重由注意力机制确定\n",
    "\n",
    "- query --> 解码器最后一层每一个时间步的隐藏状态\n",
    "\n",
    "- key --> 最后一层每一个时间步的隐藏状态\n",
    "\n",
    "- value --> 最后一层每一个时间步的隐藏状态\n",
    "\n",
    "这里 key 就是 value，原文输入序列长度有多长，就有几个键值对"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e91ba9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54db1cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305699d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, query_size, key_size, hiden_size):\n",
    "        super().__init__()\n",
    "        self.linear_q = nn.Linear(query_size, hiden_size, bias=False)\n",
    "        self.linear_k = nn.Linear(key_size, hiden_size, bias=False)\n",
    "        self.dense = nn.Linear(hiden_size, 1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    # query 形状：(batch_size, num_query, query_size)\n",
    "    # key 形状：(batch_size, num_pair, key_size)\n",
    "    # value 形状：(batch_size, num_pair, value_size)\n",
    "    def forward(self, query, key, value):\n",
    "        query_ = self.linear_q(query)   # (batch_size, num_query, hiden_size)\n",
    "        key_ = self.linear_k(key)       # (batch_size, num_pair, hiden_size)\n",
    "        H = torch.tanh(query_[:, :, None, :] + key_[:, None, :, :])      # (batch_size, num_query, num_pair, hiden_size)\n",
    "        score = self.dense(H)                     # (batch_size, num_query, num_pair, 1)\n",
    "        score = score.reshape(score.shape[0:3])   # (batch_size, num_query, num_pair)\n",
    "        weight = self.softmax(score)              # (batch_size, num_query, num_pair)\n",
    "        output = torch.bmm(weight, value)         # (batch_size, num_query, value_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c94be2a",
   "metadata": {},
   "source": [
    "编码器不做改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c1908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hiden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hiden_size, num_layers)\n",
    "\n",
    "    # 输入 x 形状：(batch_size, seq_len)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(1, 0, 2)\n",
    "        output, state = self.rnn(x)\n",
    "        # output 形状：(seq_len, batch_size, hiden_size)\n",
    "        # state 形状：(num_layers, batch_size, hiden_size)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17f27c2",
   "metadata": {},
   "source": [
    "解码器添加注意力机制，改变每次与输入 cat 的 context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c53656b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hiden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.attention = AdditiveAttention(hiden_size, hiden_size, hiden_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + hiden_size, hiden_size, num_layers)\n",
    "        self.dense = nn.Linear(hiden_size, vocab_size)\n",
    "\n",
    "    # y 的形状：(batch_size, seq_len)\n",
    "    # enc_outputs 就是编码器的返回值\n",
    "    def forward(self, y, enc_outputs):\n",
    "        output = enc_outputs[0]      # (seq_len, batch_size, hiden_size)，作为各个样本的键值对\n",
    "        state = enc_outputs[1]       # (num_layers, batch_size, hiden_size)，作为解码器 RNN 的初始隐藏状态\n",
    "\n",
    "        key_value = output.permute(1, 0, 2)   # (batch_size, num_pair, key_value_size)\n",
    "        query = state[-1]     # (batch_size, query_size)，对于每一个样本只能一个一个查询来完成\n",
    "        query = query.reshape(query.shape[0], 1, query.shape[1])    # (batch_size, num_query, query_size)，其中 num_query = 1\n",
    "\n",
    "        y = self.embedding(y).permute(1, 0, 2)    # (seq_len, batch_size, embed_size)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # 对每个样本的每一个序列进行遍历\n",
    "        for i in y:   # i 的形状：(batch_size, embed_size)\n",
    "            context = self.attention(query, key_value, key_value)  # (batch_size, num_query, key_value_size)，其中 num_query = 1\n",
    "            context = context.reshape(context.shape[0], context.shape[2])    # (batch_size, key_value_size)\n",
    "            dec_input = torch.cat((i, context), dim=1)     # (batch_size, embed_size + hiden_size)\n",
    "            dec_input = dec_input.reshape(1, dec_input.shape[0], dec_input.shape[1])   # (1, batch_size, embed_size + hiden_size)\n",
    "            dec_outputs = self.rnn(dec_input, state)\n",
    "            output = dec_outputs[0]    # (1, batch_size, hiden_size)，用于预测一个词元的隐藏状态\n",
    "            state = dec_outputs[1]     # (num_layers, batch_size, hiden_size)，更新解码器 RNN 的隐藏状态\n",
    "            query = output.permute(1, 0, 2)    # (batch_size, num_query, query_size)，其中 num_query = 1，进行下一轮查询\n",
    "            outputs.append(output)\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=0)   # (seq_len, batch_size, hiden_size)\n",
    "        return self.dense(outputs).permute(1, 0, 2)      # (batch_size, seq_len, vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
