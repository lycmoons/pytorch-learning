{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41bbf022",
   "metadata": {},
   "source": [
    "### Embedding层介绍\n",
    "\n",
    "在编码器-解码器架构中，经常将需要输入的序列数据先经过 Embedding 层处理，再交给编码器和解码器\n",
    "\n",
    "或者可以理解为 Embedding 层就是编码器、解码器中的一部分\n",
    "\n",
    "假设编码器使用 RNN 来提取特征，解码器使用另一个 RNN 来获取输出结果图解如下：\n",
    "\n",
    "![](md-img/Embedding.jpg)\n",
    "\n",
    "其中输入数据（enc_input、dec_input）使用的是词表索引表示（不是独热编码）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc812615",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Embedding层的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3f61a",
   "metadata": {},
   "source": [
    "假设现在有一个词表：\n",
    "\n",
    "```python\n",
    "{'hello': 0, 'go': 1, 'world': 2, 'hi': 3}\n",
    "```\n",
    "\n",
    "那么序列 [0, 2] 就表示 'hello world'\n",
    "\n",
    "如果直接使用这样的数据，是不合适的：\n",
    "\n",
    "- 词表中的键值对是随机的（于词表的构建过程相关）\n",
    "\n",
    "- 这样无法表示词与词之间的相关性（hello 与 hi 是同义词）\n",
    "\n",
    "Embedding 层首先将序列中的每一个词采用独热编码\n",
    "\n",
    "然后设计一个全连接层，将 vocab_size 大小的一个 token 向量映射到指定的 embedding_size 大小的向量上\n",
    "\n",
    "通过对 Embedding 层权重参数的学习，会导致相关性越强的 token 映射到的向量在空间中靠的越近（越相似）\n",
    "\n",
    "这样得到的数据可以体现出不同 token 的相关性\n",
    "\n",
    "此外，对于词表很大的数据集，表示一个 token 的代价就很大\n",
    "\n",
    "通过 Embedding 层的映射，可以将 token 信息浓缩到 embedding_size 大小的向量上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86c493",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Pytorch代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f13446be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88757e2",
   "metadata": {},
   "source": [
    "创建 Embedding 层，指定要嵌入的词表的大小，以及最终要将一个 token 映射到的向量的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e8cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30\n",
    "embedding_size = 10\n",
    "embedding = nn.Embedding(vocab_size, embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1c073",
   "metadata": {},
   "source": [
    "输入数据形状为 (batch_size, seq_len)，其中每一个 token 使用词表中对应的索引表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1be3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25, 13,  7,  3, 20],\n",
       "        [15,  3,  9, 26,  6],\n",
       "        [15,  7,  2, 10,  1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "seq_len = 5\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f2cd9",
   "metadata": {},
   "source": [
    "最终得到的输出形状为 (batch_size, seq_len, embedding_size)，每一个 token 使用一个向量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f370ddb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = embedding(x)\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
