{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c32c5ae",
   "metadata": {},
   "source": [
    "### 注意力机制介绍\n",
    "\n",
    "当我们需要针对当前的上下文输出某些结果时，我们可能会应用到注意力机制，即需要输出的结果只与特定的某个上下文相关\n",
    "\n",
    "比如我们在做英语的阅读理解选择题，问题问的是小明几点回家的，那么我们在原文中找答案时只需要注意原文与时间相关的内容即可\n",
    "\n",
    "而不是读完整篇文章再进行作答（当文章比较长的时候，这样做我们可能会遗忘答案）\n",
    "\n",
    "注意力机制的关键点：\n",
    "\n",
    "- 先读问题，再看原文\n",
    "\n",
    "- 注意力放在与问题相关的原文上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa25dd80",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 三个关键量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e381159",
   "metadata": {},
   "source": [
    "在注意力机制中，有三个重要的量：\n",
    "\n",
    "- query：查询，可以理解为阅读理解的选择题（问题）\n",
    "\n",
    "- key：键，可以理解为阅读理解的原文每个部分的标签（时间、人物、）\n",
    "\n",
    "- value：值，可以理解为每个标签的具体内容（10点、小明）\n",
    "\n",
    "其中 key 和 value 是成对出现的，由所有的键值对组成查询所在的上下文环境\n",
    "\n",
    "注意力机制相当于我们在回答 query 这个问题时，找到 query 与每一个 key 的相关性\n",
    "\n",
    "依据 query 与每一个 key 的相关性，确定最终输出时使用每一个 value 的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0cae6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 向量表示形式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d725362",
   "metadata": {},
   "source": [
    "我们将每一个 query、key、value 都用向量表示\n",
    "\n",
    "那么有如下数据形式：\n",
    "\n",
    "- Q --> (num_query, query_size)\n",
    "\n",
    "- K --> (num_pair, key_size)\n",
    "\n",
    "- V --> (num_pair, value_size)\n",
    "\n",
    "其中：\n",
    "\n",
    "- num_query：查询的个数（问题的个数）\n",
    "\n",
    "- num_pair：键值对的个数（原文尺寸）\n",
    "\n",
    "- query_size：用来表示一个查询的向量的尺寸\n",
    "\n",
    "- key_size：用来表示一个键的向量的尺寸\n",
    "\n",
    "- value_size：用来表示一个值的向量的尺寸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbc8b9",
   "metadata": {},
   "source": [
    "每一个输入的样本有 num_pair 个键值对（上下文环境）\n",
    "\n",
    "可以在每一个样本上进行 num_query 次查询（输出）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae48cc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 含参注意力（简单情况下）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dcf233",
   "metadata": {},
   "source": [
    "含参注意力一般是加性注意力，通过设置学习参数将 query 和 key 用加法融合，学习这两个量之间的相关性\n",
    "\n",
    "最终得到每一个查询的输出所对应的每一个 value 的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9735c",
   "metadata": {},
   "source": [
    "从最简单的情况来考虑，在一个样本上进行一次查询（一个输出），图解如下：\n",
    "\n",
    "- Q --> 在一个样本上的一次查询\n",
    "\n",
    "- (Ki, Vi) --> 一个样本的键值对（表示这样本的特征）\n",
    "\n",
    "K、Q、V 向量的维度可能各不相同\n",
    "\n",
    "![](md-img/加性注意力机制.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ce07f",
   "metadata": {},
   "source": [
    "上面的箭头表示全连接，且是没有偏置的全连接\n",
    "\n",
    "在通过查询与每一个键之间的相关性得到注意力分数后，通过一层 softmax，即可得到注意力权重\n",
    "\n",
    "最终的输出就是键值对中的值与注意力权重的线性组合：\n",
    "\n",
    "```python\n",
    "output = W1 * V1 + W2 * V2 + ··· + Wn * Vn\n",
    "```\n",
    "\n",
    "其中 W 是标量，V 是向量，最终的输出结果是与 V 同样维度的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451e51b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 含参注意力（复杂情况下）\n",
    "\n",
    "当在多个样本上，分别进行多次查询时，就是比较复杂的情况\n",
    "\n",
    "此时各个矩阵形状如下：\n",
    "\n",
    "- Q --> (batch_size, num_query, query_size)\n",
    "\n",
    "- K --> (batch_size, num_pair, key_size)\n",
    "\n",
    "- V --> (batch_size, num_pair, value_size)\n",
    "\n",
    "此处 K、V 无需下标，足以表示每一个样本的所有键值对\n",
    "\n",
    "底层处理原理就是简单情况下的处理，现在考虑怎么将其转化成矩阵计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f72cf853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b09a0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 的形状为 torch.Size([30, 10, 20])\n",
      "K 的形状为 torch.Size([30, 15, 8])\n",
      "V 的形状为 torch.Size([30, 15, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "num_query = 10\n",
    "query_size = 20\n",
    "num_pair = 15\n",
    "key_size = 8\n",
    "value_size = 5\n",
    "\n",
    "Q = torch.randn(batch_size, num_query, query_size)\n",
    "K = torch.randn(batch_size, num_pair, key_size)\n",
    "V = torch.randn(batch_size, num_pair, value_size)\n",
    "print('Q 的形状为 {}'.format(Q.shape))\n",
    "print('K 的形状为 {}'.format(K.shape))\n",
    "print('V 的形状为 {}'.format(V.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cdaa284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_ 的形状为 torch.Size([30, 10, 40])\n",
      "K_ 的形状为 torch.Size([30, 15, 40])\n"
     ]
    }
   ],
   "source": [
    "# 将 Q 和 K 映射到同一个向量维度\n",
    "hiden_size = 40\n",
    "linear_q = nn.Linear(query_size, hiden_size, bias=False)\n",
    "linear_k = nn.Linear(key_size, hiden_size, bias=False)\n",
    "\n",
    "Q_ = linear_q(Q)\n",
    "K_ = linear_k(K)\n",
    "\n",
    "print('Q_ 的形状为 {}'.format(Q_.shape))\n",
    "print('K_ 的形状为 {}'.format(K_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7055a821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_ 的形状为 torch.Size([30, 10, 1, 40])\n",
      "K_ 的形状为 torch.Size([30, 1, 15, 40])\n",
      "H 的形状为 torch.Size([30, 10, 15, 40])\n"
     ]
    }
   ],
   "source": [
    "# 每一个查询要与对应样本的每一个键做向量加法\n",
    "# 希望得到的隐藏层结果形状为 (batch_size, num_query, num_pair, hiden_size)\n",
    "# 需要为 Q_ 和 K_ 增加一个维度，并通过广播机制实现向量加法\n",
    "\n",
    "Q_ = Q_[:, :, None, :]     # 在目标位置增加一个 num_pair 维度\n",
    "K_ = K_[:, None, :, :]     # 在目标位置增加一个 num_query 维度\n",
    "\n",
    "print('Q_ 的形状为 {}'.format(Q_.shape))\n",
    "print('K_ 的形状为 {}'.format(K_.shape))\n",
    "\n",
    "H = torch.tanh(Q_ + K_)\n",
    "print('H 的形状为 {}'.format(H.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f468a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S 的形状为 torch.Size([30, 10, 15])\n"
     ]
    }
   ],
   "source": [
    "# 将表示每一个查询与每一个键的相关性的向量全连接为一个标量\n",
    "# 即计算注意力分数\n",
    "linear_s = nn.Linear(hiden_size, 1, bias=False)\n",
    "S = linear_s(H).reshape(batch_size, num_query, num_pair)\n",
    "print('S 的形状为 {}'.format(S.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3000195b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W 的形状为 torch.Size([30, 10, 15])\n"
     ]
    }
   ],
   "source": [
    "# 调整注意力分数矩阵的形状\n",
    "# 并对每一行应用 softmax\n",
    "softmax = nn.Softmax(dim=2)\n",
    "W = softmax(S)\n",
    "print('W 的形状为 {}'.format(W.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b2fc1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 的形状为 torch.Size([30, 10, 5])\n"
     ]
    }
   ],
   "source": [
    "# 根据注意力权重和键值对中的值获取最终的输出\n",
    "# 按照每一个样本分别做矩阵乘法即可\n",
    "# torch.bmm 就是按照批次，对应的进行矩阵计算\n",
    "output = torch.bmm(W, V)\n",
    "print('output 的形状为 {}'.format(output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01fa7eb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 整合上述代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aead0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, query_size, key_size, hiden_size):\n",
    "        super().__init__()\n",
    "        self.linear_q = nn.Linear(query_size, hiden_size, bias=False)\n",
    "        self.linear_k = nn.Linear(key_size, hiden_size, bias=False)\n",
    "        self.dense = nn.Linear(hiden_size, 1, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    # query 形状：(batch_size, num_query, query_size)\n",
    "    # key 形状：(batch_size, num_pair, key_size)\n",
    "    # value 形状：(batch_size, num_pair, value_size)\n",
    "    def forward(self, query, key, value):\n",
    "        query_ = self.linear_q(query)   # (batch_size, num_query, hiden_size)\n",
    "        key_ = self.linear_k(key)       # (batch_size, num_pair, hiden_size)\n",
    "        H = torch.tanh(query_[:, :, None, :] + key_[:, None, :, :])      # (batch_size, num_query, num_pair, hiden_size)\n",
    "        score = self.dense(H).reshape(batch_size, num_query, num_pair)   # (batch_size, num_query, num_pair)\n",
    "        weight = self.softmax(score)          # (batch_size, num_query, num_pair)\n",
    "        output = torch.bmm(weight, value)     # (batch_size, num_query, value_size)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
