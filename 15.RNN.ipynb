{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0d25ba",
   "metadata": {},
   "source": [
    "### RNN模型纵览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b68ec",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 训练中的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10c4793",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d420d397",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 梯度裁剪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d2f4c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 潜变量初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b718404",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 使用模型进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884cc7c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1a25bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional\n",
    "\n",
    "\n",
    "class RNNModel:\n",
    "    # hiden_size 是潜变量的大小\n",
    "    def __init__(self, time_step, vocab_size, hiden_size):\n",
    "        self.time_step = time_step\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hiden_size = hiden_size\n",
    "\n",
    "        # 用于更新潜变量的模型参数\n",
    "        self.w_hh = torch.randn(hiden_size, hiden_size) * 0.01\n",
    "        self.w_xh = torch.randn(vocab_size, hiden_size) * 0.01\n",
    "        self.b_h = torch.zeros(hiden_size)\n",
    "\n",
    "        # 通过潜变量获取输出的模型参数\n",
    "        self.w_y = torch.randn(hiden_size, vocab_size) * 0.01\n",
    "        self.b_y = torch.randn(vocab_size) * 0.01\n",
    "\n",
    "        self.w_hh.requires_grad_(True)\n",
    "        self.w_xh.requires_grad_(True)\n",
    "        self.b_h.requires_grad_(True)\n",
    "        self.w_y.requires_grad_(True)\n",
    "        self.b_y.requires_grad_(True)\n",
    "\n",
    "\n",
    "    # 在没有任何历史信息的情况下，初始化潜变量（全部初始化为 0）\n",
    "    def init_state(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hiden_size)\n",
    "\n",
    "    # 对一个批次的样本进行正向传播\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    # X 的形状为 (time_step, batch_size, vocab_size)\n",
    "    # 这样可以同时更新不同样本的潜变量\n",
    "    def forward(self, X):\n",
    "        h = self.init_state(X.shape[1])    # 初始化潜变量\n",
    "        Y = []     # 用于保存每一个时间步长上的预测值\n",
    "        for x in X:\n",
    "            h = h @ self.w_hh + x @ self.w_xh + self.b_h\n",
    "            y = h @ self.w_y + self.b_y\n",
    "            Y.append(y)\n",
    "        return torch.tensor(Y), (h,)\n",
    "    \n",
    "    # 梯度裁剪，防止梯度爆炸，只有整体梯度的 L2 范数大于 theta 时，才需要进行裁剪\n",
    "    def grad_clipping(self, theta):\n",
    "        norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in self.get_parameters()))\n",
    "        if norm > theta:\n",
    "            for param in self.get_parameters():\n",
    "                param.grad[:] *= theta / norm\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return [self.w_hh, self.w_xh, self.b_h, self.w_y, self.b_y]\n",
    "    \n",
    "    # 预测函数，根据 num_prefix 个 token 的输入，预测后续 num_predic 个 token 的输出\n",
    "    # prefix 的形状为 (len, vocab_size)\n",
    "    def prefict(self, prefix, num_predic):\n",
    "        # 潜变量的预热\n",
    "        h = self.init_state(1)\n",
    "        for x in prefix:\n",
    "            h = h @ self.w_hh + x.reshape(1, -1) @ self.w_xh + self.b_h\n",
    "        \n",
    "        # 进行后续 num_predic 个词的预测\n",
    "        Y = []\n",
    "        for _ in range(num_predic):\n",
    "            y = (h @ self.w_y + self.b_y).flatten().argmax()\n",
    "            h = h @ self.w_hh + functional.one_hot(y, self.vocab_size).reshape(1, -1) @ self.w_xh + self.b_h\n",
    "            Y.append(y)\n",
    "        return torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdfc6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数的计算\n",
    "# y_hat 和 y 的形状都是 (time_step, batch_size, vocab_size)\n",
    "# 每个样本在不同的时间步长上都有输出（vocab_size 个得分）\n",
    "def loss_func(y_hat, y):\n",
    "    # 将数据形状变为(time_step * batch_size, vocab_size)\n",
    "    # 方便计算 softmax 和 crossEntropyLoss\n",
    "    y1 = torch.cat(y_hat, dim=0)\n",
    "    y2 = torch.cat(y, dim=0)\n",
    "    return torch.nn.CrossEntropyLoss(y1, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76a12bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行一次参数更新（训练一个 batch）\n",
    "def train_batch(rnn_model: RNNModel, X:torch.Tensor, Y:torch.Tensor, optimizer:torch.optim.Optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    y, _ = rnn_model.forward(X)\n",
    "    loss = loss_func(y, Y)\n",
    "    loss.backword()\n",
    "    rnn_model.grad_clipping(1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9614c6d9",
   "metadata": {},
   "source": [
    "然后使用不同的batch循环调用上面的训练即可"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
