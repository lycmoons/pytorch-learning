{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "690938ed",
   "metadata": {},
   "source": [
    "### 机器翻译网络总览"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f253ad",
   "metadata": {},
   "source": [
    "使用编码器-解码器架构的 seq to seq 模型图解如下：\n",
    "\n",
    "![](md-img\\机器翻译.jpg)\n",
    "\n",
    "其中编码器使用 RNN 读取输入的 seq 的特征，解码器使用另一个 RNN 输出预测的 seq\n",
    "\n",
    "编码器和解码器的 RNN 层数以及隐藏层的大小是一样的\n",
    "\n",
    "<br>\n",
    "\n",
    "使用编码器最后一个时间步输出的各层隐藏状态来初始化解码器 RNN 的隐藏状态（粉色箭头）\n",
    "\n",
    "解码器中 RNN 的输入使用的是标签 seq 与编码器最后一个时间步最后一层隐藏状态合并输入（红色箭头）\n",
    "\n",
    "<br>\n",
    "\n",
    "此外还需要对特征 seq 以及标签 seq 做预处理，特征 seq 尾部添加 '\\<eos>'，标签 seq 头部添加 '\\<bos>'\n",
    "\n",
    "且输入的 seq 只是将 token 转为 数值（索引），不需要使用独热编码\n",
    "\n",
    "如下图所示：\n",
    "\n",
    "![](md-img\\机器翻译2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e074a83",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054249f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b005165",
   "metadata": {},
   "source": [
    "编码器设计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "989cf119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hiden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hiden_size, num_layers)\n",
    "\n",
    "    # 输入 x 形状：(batch_size, seq_len)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(1, 0, 2)\n",
    "        output, state = self.rnn(x)\n",
    "        # output 形状：(seq_len, batch_size, hiden_size)\n",
    "        # state 形状：(num_layers, batch_size, hiden_size)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71afcbad",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "解码器设计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f22d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hiden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + hiden_size, hiden_size, num_layers)\n",
    "        self.dense = nn.Linear(hiden_size, vocab_size)\n",
    "\n",
    "    # 输入 y 形状：(batch_size, seq_len)\n",
    "    # 输入 init_state 形状：(num_layers, batch_size, hiden_size)\n",
    "    # 输入 context 形状为：(batch_size, hiden_size)，一般是编码器最后一个时间步、最后一层的隐藏状态\n",
    "    def forward(self, y, init_state, context):\n",
    "        y = self.embedding(y).permute(1, 0, 2)\n",
    "        context = context.repeat(y.shape[0], 1, 1)\n",
    "        y_and_context = torch.cat((y, context), dim=2)\n",
    "        output, state = self.rnn(y_and_context, init_state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # output 形状：(batch_size, seq_len, vocab_size)\n",
    "        # state 形状：(num_layers, batch_size, hiden_size)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb605a66",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "整合编码器和解码器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a5b20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqToSeqModel(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        _, state = self.encoder(x)\n",
    "        return self.decoder(y, state, state[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950fc379",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "损失函数设计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "062f793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights 的形状：(batch_size, seq_len)，表示每一个词元损失的权重\n",
    "# valid_len 的形状：(batch_size,)\n",
    "\n",
    "# 设计函数，用于屏蔽填充项的损失，获取每一个词元损失的权重\n",
    "def seq_mask(weights, valid_len, value=0):\n",
    "    seq_len = weights.shape[1]\n",
    "    mask = torch.arange(seq_len)[None, :] < valid_len[:, None]\n",
    "    # mask 的形状：(batch_size, seq_len)，保留部分为 True，不保留部分为 False\n",
    "\n",
    "    weights[~mask] = value     # 选出 False 在的地方进行赋值\n",
    "    # 舍弃一个填充词元的损失等于舍弃一整排vocab_size\n",
    "    return weights\n",
    "\n",
    "\n",
    "# 改写 CrossEntropyLoss 类，为其屏蔽填充项\n",
    "class CELossWithMask(nn.CrossEntropyLoss):\n",
    "    # 改写 forward 函数\n",
    "    # 模型输出的 pred 形状：(batch_size, seq_len, vocab_size)\n",
    "    # label 的形状：(batch_size, seq_len)，非独热编码\n",
    "    # valid_len 的形状：(batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)    # 初始权重均为 1\n",
    "        weights = seq_mask(weights, valid_len)\n",
    "\n",
    "        # 设置不对每一个词元损失进行累加\n",
    "        self.reduction = 'none'\n",
    "\n",
    "        unweighted_loss = super(CELossWithMask, self).forward(pred.permute(0, 2, 1), label)\n",
    "        # unweighted_loss 的形状：(batch_size, seq_len)，表示每个词元的损失\n",
    "\n",
    "        weighted_loss = unweighted_loss * weights\n",
    "\n",
    "        # 返回每一个样本的损失，形状为：(batch_size,)\n",
    "        return weighted_loss.mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c55ba6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "训练过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a606a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "def train(model: SeqToSeqModel, data_loader: DataLoader, lr, num_epochs, device):\n",
    "    model.to(device)\n",
    "\n",
    "    # 损失函数与优化器的选择\n",
    "    loss_func = CELossWithMask().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(num_epochs):\n",
    "        for x, y, y_valid_len in data_loader:\n",
    "            x.to(device)\n",
    "            y.to(device)\n",
    "            y_valid_len.to(device)\n",
    "\n",
    "            # 损失的计算\n",
    "            optimizer.zero_grad()\n",
    "            y_hat, _ = model(x, y)\n",
    "            loss = loss_func(y_hat, y, y_valid_len).sum()\n",
    "\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "\n",
    "            # 梯度裁剪与梯度更新\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1, norm_type=2)\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac3178",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "使用模型进行预测的过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f74752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测时，编码器处理与训练时一致（提取特征）\n",
    "# 解码器进行预测时，使用前一个时间步的预测词元作为下一个时间步的输入\n",
    "# x 是一个样本形状为：(seq_len,)\n",
    "# token_to_idx 是输出序列的词表\n",
    "# max_len 是输出序列的最大长度\n",
    "def predict(net: SeqToSeqModel, x, token_to_idx, max_len):\n",
    "    _, state = net.encoder(x.reshape(1, -1))\n",
    "    context = state[-1]\n",
    "    dec_input = torch.tensor([token_to_idx['<bos>']]).reshape(1, 1)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        output, state = net.decoder(dec_input, state, context)\n",
    "        dec_input = output.argmax(dim=2)\n",
    "        pred = dec_input[0][0].item()\n",
    "\n",
    "        if pred == token_to_idx['<eos>']:\n",
    "            break\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "# 该预测函数用于预测一个样本"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
