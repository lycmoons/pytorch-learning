{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b179d34",
   "metadata": {},
   "source": [
    "### 自注意力\n",
    "\n",
    "自注意力机制指的是对于一个序列样本，将其每一个token即看作键也看做值（用来表示序列的键值对）\n",
    "\n",
    "同时将序列中每一个 token 视为对该序列的一个查询\n",
    "\n",
    "```\n",
    "序列：(batch_size, seq_len, embed_size)\n",
    "\n",
    "键/值：(batch_size, seq_len, embed_size)，其中 seq_len = num_pairs，embed_size = key_value_size\n",
    "\n",
    "查询：(batch_size, seq_len, embed_size)，其中 seq_len = num_query，embed_size = query_size\n",
    "```\n",
    "\n",
    "也就是说，自注意力中 Q、K、V 矩阵的形状一样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b0b23",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 自注意力输出的意义\n",
    "\n",
    "自注意力的输出形状：(batch_size, seq_len, embed_size)\n",
    "\n",
    "该输出相当于对原序列做了个变换，仍然用来表示序列信息\n",
    "\n",
    "相比原来的序列来说，每一个 token 都考虑了整个序列的上下文信息\n",
    "\n",
    "相当于获取了每一个 token 在特定的上下文中的独特意思（同一个词在不同的话中表达的意思可能不同）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dfa9d5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 位置编码的必要性\n",
    "\n",
    "上述的自注意力机制中，其实有一定的缺陷。\n",
    "\n",
    "键值对（每一个 token）在使用的时候其实是无序的，而序列本身的每一个 token 是有序的\n",
    "\n",
    "我们需要为一个序列中每一个 token 添加其位置信息\n",
    "\n",
    "该操作（嵌入位置编码）在序列进行 embedding 后就进行\n",
    "\n",
    "嵌入位置信息后再进行自注意力的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484d29a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 经典正弦余弦位置编码（固定编码）\n",
    "\n",
    "位置编码公式如下：\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i / d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i / d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $pos$ 表示序列中的位置（每一个 token 在序列中的位置索引）\n",
    "\n",
    "- $i$ 表示维度索引（用来表示一个 token 的向量的各个维度）\n",
    "\n",
    "- $d_{model}$ 是嵌入向量维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68abe5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    # x 的形状：(batch_size, seq_len, embed_size)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_size = x.shape\n",
    "\n",
    "        # 获取表示矩阵中每一个元素所属的 token 在序列中的索引\n",
    "        pos_map = torch.arange(seq_len).unsqueeze(1)      # (seq_len, 1)\n",
    "        pos_map = pos_map.expand(seq_len, embed_size)     # (seq_len, embed_size)\n",
    "\n",
    "\n",
    "        # 获取表示矩阵中每个元素在表示 token 的向量中的维度\n",
    "        dim_map = torch.arange(embed_size).unsqueeze(0)      # (1, embed_size)\n",
    "        dim_map = dim_map.expand(seq_len, embed_size)        # (seq_len, embed_size)\n",
    "\n",
    "        # 计算位置编码矩阵\n",
    "        angle = pos_map / torch.pow(10000, 2 * dim_map / embed_size)    # (seq_len, embed_size)\n",
    "        pe = torch.zeros(seq_len, embed_size)       # (seq_len, embed_size)\n",
    "        pe[:, 0::2] = torch.sin(angle[:, 0::2])     # dim 为偶数，使用 sin\n",
    "        pe[:, 1::2] = torch.cos(angle[:, 1::2])     # dim 为奇数，使用 cos\n",
    "\n",
    "        # 每个样本对应位置的位置编码是一致的\n",
    "        pe = pe.unsqueeze(0).expand(batch_size, seq_len, embed_size)    # (batch_size, seq_len, embed_size)\n",
    "\n",
    "        return x + pe   # 嵌入位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a9059",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 含位置编码的自注意力代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c09e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, hiden_size):\n",
    "        super().__init__()\n",
    "        self.hiden_size = torch.tensor(hiden_size)\n",
    "        self.position_encoder = PositionEncoder()\n",
    "        self.linear_q = nn.Linear(embed_size, hiden_size)\n",
    "        self.linear_k = nn.Linear(embed_size, hiden_size)\n",
    "        self.linear_v = nn.Linear(embed_size, hiden_size)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "\n",
    "    # x 的形状：(batch_size, seq_len, embed_size)\n",
    "    def forward(self, x):\n",
    "        x = self.position_encoder(x)     # (batch_size, seq_len, embed_size)\n",
    "        query = self.linear_q(x)         # (batch_size, seq_len, hiden_size)\n",
    "        key = self.linear_q(x)           # (batch_size, seq_len, hiden_size)\n",
    "        value = self.linear_q(x)         # (batch_size, seq_len, hiden_size)\n",
    "        score = torch.bmm(query, key.permute(0, 2, 1)) / torch.sqrt(self.hiden_size)   # (batch_size, seq_len, seq_len)\n",
    "        weight = self.softmax(score)     # (batch_size, seq_len, seq_len)\n",
    "        return torch.bmm(weight, value)  # (batch_size, seq_len, hiden_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
