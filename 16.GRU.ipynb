{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2fb021",
   "metadata": {},
   "source": [
    "### GRU总览\n",
    "\n",
    "GRU 于普通的 RNN 的区别在于对于潜变量的更新\n",
    "\n",
    "GRU 在对潜变量进行更新时会考虑对历史序列信息的保留程度（这个部分也需要设计为模型参数进行学习）\n",
    "\n",
    "引入了更新门、重置门和候选潜变量的概念，这三个可以理解为用于计算最终更新的潜变量的中间值\n",
    "\n",
    "更新门（Update Gate）：\n",
    "$$\n",
    "z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z)\n",
    "$$\n",
    "\n",
    "重置门（Reset Gate）：\n",
    "$$\n",
    "r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r)\n",
    "$$\n",
    "\n",
    "候选隐藏状态（Candidate Hidden State）：\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h x_t + U_h (r_t \\odot h_{t-1}) + b_h)\n",
    "$$\n",
    "\n",
    "最终隐藏状态（New Hidden State）：\n",
    "$$\n",
    "h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96090ee3",
   "metadata": {},
   "source": [
    "使用 GRU 更新一次潜变量的图解如下：\n",
    "\n",
    "![](md-img/GRU.jpg)\n",
    "\n",
    "其中箭头表示全连接\n",
    "\n",
    "对于更新门和重置门的激活函数使用 Sigmoid 函数\n",
    "\n",
    "对于候选潜变量的激活函数使用 tanh 函数\n",
    "\n",
    "使用最终得到的潜变量即可进行下一个 token 的预测（同 RNN）\n",
    "\n",
    "$$\n",
    "其中 H、Z、R、\\tilde{H} 形状一致\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a1425f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 代码从零实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class GRUModel:\n",
    "    # 根据输入的词表大小和指定的隐藏层大小来初始化模型参数（要保留梯度）\n",
    "    def __init__(self, vocab_size, hiden_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hiden_size = hiden_size\n",
    "\n",
    "        self.w_hz = torch.randn(hiden_size, hiden_size) * 0.01\n",
    "        self.w_xz = torch.randn(vocab_size, hiden_size) * 0.01\n",
    "        self.b_z = torch.zeros(hiden_size)\n",
    "        self.w_hr = torch.randn(hiden_size, hiden_size) * 0.01\n",
    "        self.w_xr = torch.randn(vocab_size, hiden_size) * 0.01\n",
    "        self.b_r = torch.zeros(hiden_size)\n",
    "        self.w_hrh = torch.randn(hiden_size, hiden_size) * 0.01\n",
    "        self.w_xh = torch.randn(vocab_size, hiden_size) * 0.01\n",
    "        self.b_h = torch.zeros(hiden_size)\n",
    "        self.w_hy = torch.randn(hiden_size, vocab_size) * 0.01\n",
    "        self.b_y = torch.zeros(vocab_size)\n",
    "\n",
    "        self.parameters = [self.w_hz, self.w_xz, self.b_z,\n",
    "                           self.w_hr, self.w_xr, self.b_r,\n",
    "                           self.w_hrh, self.w_xh, self.b_h,\n",
    "                           self.w_hy, self.b_y]\n",
    "        \n",
    "        \n",
    "        for param in self.parameters:\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "    # 外界获取当前模型参数\n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "    # 此处指定输入的 X 的形状为 (time_step, batch_size, vocab_size)\n",
    "    def forward(self, X):\n",
    "        h = torch.zeros(x.shape[1], self.hiden_size)   # 初始化隐藏状态\n",
    "\n",
    "        Y = []   # 用于保存所有的预测输出\n",
    "\n",
    "        # 按找时间步长，往后推算每一个样本的潜变量\n",
    "        for x in X:\n",
    "            z = torch.sigmoid(x @ self.w_xz + h @ self.w_hz + self.b_z)        # 更新门\n",
    "            r = torch.sigmoid(x @ self.w_xr + h @ self.w_hr + self.b_r)        # 重置门\n",
    "            h_ = torch.tanh(x @ self.w_xh + (h * r) @ self.w_hrh + self.b_h)   # 候选隐藏状态\n",
    "\n",
    "            h = z * h_ + (1 - z) * h    # 更新隐藏状态\n",
    "\n",
    "            # 使用隐藏状态获取最终输出\n",
    "            output = h @ self.w_hy + self.b_y\n",
    "            Y.append(output)\n",
    "\n",
    "        # 此处返回的 Y 的形状为 (time_step * batch_size, vocab_size)，方便计算损失函数\n",
    "        return torch.cat(Y, dim=0), (h,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
