{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6da01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45cd135",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 全连接层（线性层）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60fe4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定输入以及输出神经元的数量，创建全连接层\n",
    "linear = torch.nn.Linear(in_features=10, out_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9355f31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.0455,  0.3008,  0.1281,  0.2621,  0.0457,  0.3075, -0.2405,  0.0377,\n",
       "          -0.1661, -0.2990],\n",
       "         [-0.2488,  0.2213, -0.3131,  0.1024, -0.2728,  0.2503, -0.2249, -0.0123,\n",
       "          -0.1601, -0.1512],\n",
       "         [-0.2920,  0.0107, -0.3029,  0.2823,  0.1365, -0.2969,  0.2674,  0.1127,\n",
       "          -0.2489,  0.2587],\n",
       "         [ 0.0882,  0.0691, -0.0028,  0.0456, -0.2048,  0.2760,  0.1824, -0.0427,\n",
       "          -0.0581,  0.0818],\n",
       "         [-0.0637,  0.2225, -0.0490,  0.2941,  0.0802,  0.1224,  0.0058,  0.0408,\n",
       "           0.2100, -0.1777]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0460,  0.3107, -0.1693, -0.1184, -0.0231], requires_grad=True))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建好全连接层后，会自动初始化权重与偏置\n",
    "linear.weight, linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bab0109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# 调用全连接层的正向传播方法（从输入到输出）\n",
    "x = torch.arange(30, dtype=torch.float32).reshape(3, 10)    # 3 个样本，每个样本 10 个输入\n",
    "output = linear(x)                                          # 通过调用 call 魔法方法即可实现正向传播，获取输出\n",
    "print(output.shape)                                         # 分别得到 3 个样本的输出，每个样本对应 5 个输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97b044",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### ReLU层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6083d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接创建 ReLU 层，不需要任何参数\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# 该层也没有任何模型参数，只是单纯的对输入的任何元素施加 ReLU 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc681f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 调用 ReLU 层\n",
    "x = torch.tensor([1, 2, -3, -0.5])\n",
    "output = relu(x)                    # 也是通过调用 call 魔法方法来实现正向传播\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f50b15",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Sigmoid层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd6eb022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7311, 0.8808, 0.0474, 0.3775])\n"
     ]
    }
   ],
   "source": [
    "# 整体使用方法与 ReLU 层类似\n",
    "sigmoid = torch.nn.Sigmoid()          # 创建 Sigmoid 层\n",
    "x = torch.tensor([1, 2, -3, -0.5])    # 测试样本\n",
    "output = sigmoid(x)                   # 通过调用 call 魔法方法实现正向传播\n",
    "print(output)                         # 相当于对每一个输入元素调用 Sigmoid 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d0455",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Softmax层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af243567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定应用 Softmax 函数的维度，并创建 Softmax 层\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "# 一般认为一行表示一个样本，对每一行应用 Softmax 函数（dim = 1）\n",
    "# 即最终各行元素之和均为 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f9f4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0341, 0.0603, 0.1581, 0.0621, 0.5418, 0.1436],\n",
      "        [0.1167, 0.1364, 0.2221, 0.1870, 0.2396, 0.0981],\n",
      "        [0.0561, 0.1891, 0.1158, 0.2129, 0.1480, 0.2782]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 6)    # 3 个样本，6 个输出（在分类任务中表示对 6 个类别的输出）\n",
    "output = softmax(x)      # 在每一行元素上应用 Softmax 函数（得到对应 6 个类别的概率）\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3712b7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sum(dim=1)      # 各行元素之和均为 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95f408",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3552304",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "卷积运算用的最多的是二维卷积（对图像进行处理）\n",
    "其中输入、输出数据的维度顺序为 (N, C, H, W)\n",
    "\n",
    "in_channels    -- 输入数据的通道数，也是核的通道数\n",
    "out_channels   -- 输出数据的通道数，也是核的个数\n",
    "kernel_size    -- 核的大小\n",
    "stride         -- 卷积运算的步幅\n",
    "padding        -- 对输入数据的填充\n",
    "'''\n",
    "\n",
    "# 通过指定卷积层的参数来创建卷积层\n",
    "convolution = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb6479d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[[[-0.0218,  0.0320,  0.1615],\n",
       "           [-0.0820, -0.1020, -0.1386],\n",
       "           [-0.0526,  0.1307, -0.1749]],\n",
       " \n",
       "          [[-0.0671, -0.1082,  0.1593],\n",
       "           [-0.1356, -0.0759,  0.1299],\n",
       "           [-0.1051,  0.1729,  0.1365]],\n",
       " \n",
       "          [[-0.0749,  0.0309,  0.1770],\n",
       "           [-0.0815, -0.0419,  0.0655],\n",
       "           [ 0.0554,  0.1530,  0.0756]]],\n",
       " \n",
       " \n",
       "         [[[-0.1360,  0.0424, -0.0903],\n",
       "           [-0.1453, -0.1251, -0.1236],\n",
       "           [ 0.0097, -0.1253, -0.1711]],\n",
       " \n",
       "          [[-0.0698,  0.0930, -0.1879],\n",
       "           [-0.1744, -0.0228,  0.0995],\n",
       "           [ 0.0508, -0.1039, -0.1312]],\n",
       " \n",
       "          [[ 0.1577,  0.1495,  0.1759],\n",
       "           [ 0.1540,  0.0169,  0.0583],\n",
       "           [-0.1777,  0.0854,  0.1430]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0044,  0.0039, -0.1510],\n",
       "           [-0.1274, -0.0105,  0.0777],\n",
       "           [ 0.0142, -0.1594, -0.1210]],\n",
       " \n",
       "          [[-0.0354, -0.1450, -0.0793],\n",
       "           [-0.1736,  0.1643,  0.0766],\n",
       "           [-0.0761, -0.1523,  0.1465]],\n",
       " \n",
       "          [[ 0.1305,  0.0759,  0.0293],\n",
       "           [ 0.0067, -0.0162,  0.0138],\n",
       "           [ 0.1237, -0.0307,  0.0109]]],\n",
       " \n",
       " \n",
       "         [[[-0.1497,  0.1597,  0.0135],\n",
       "           [ 0.1502, -0.0829,  0.1883],\n",
       "           [ 0.0798, -0.0541, -0.1135]],\n",
       " \n",
       "          [[-0.1321,  0.0713, -0.1069],\n",
       "           [ 0.1654,  0.0521, -0.1231],\n",
       "           [-0.0891, -0.1641, -0.0534]],\n",
       " \n",
       "          [[-0.0919, -0.1408,  0.0438],\n",
       "           [-0.0049, -0.0692,  0.0151],\n",
       "           [ 0.0786, -0.0031,  0.0562]]],\n",
       " \n",
       " \n",
       "         [[[-0.0971, -0.0990, -0.0842],\n",
       "           [ 0.0859, -0.0165, -0.0973],\n",
       "           [-0.1839, -0.1624,  0.1289]],\n",
       " \n",
       "          [[ 0.0419,  0.0653, -0.1363],\n",
       "           [ 0.0496,  0.1417,  0.0564],\n",
       "           [ 0.0236,  0.0418,  0.0740]],\n",
       " \n",
       "          [[-0.1404, -0.1111, -0.0645],\n",
       "           [ 0.1144, -0.0384,  0.0155],\n",
       "           [-0.0524, -0.1259,  0.0434]]],\n",
       " \n",
       " \n",
       "         [[[-0.1037, -0.1267, -0.0924],\n",
       "           [ 0.0042,  0.1409, -0.1762],\n",
       "           [-0.1186,  0.1490, -0.1751]],\n",
       " \n",
       "          [[-0.0600, -0.0245,  0.0317],\n",
       "           [-0.0260,  0.1392,  0.0500],\n",
       "           [-0.0891, -0.0323, -0.0947]],\n",
       " \n",
       "          [[-0.0120,  0.1696,  0.1682],\n",
       "           [-0.1596,  0.0919, -0.1729],\n",
       "           [-0.1484, -0.0991, -0.0469]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1535, -0.1141,  0.1199, -0.1747,  0.0410,  0.1079],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建好卷积层后，会自动初始化好模型参数（卷积核 和 偏置）\n",
    "convolution.weight, convolution.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8de5940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 32, 32)     # 模拟 1 个大小为 32 * 32 的 3 通道（RGB）彩色图像数据\n",
    "output = convolution(x)           # 通过调用 call 魔法方法进行卷积层的正向传播\n",
    "print(output.shape)               # 可以得到通道数等于核的个数的新数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6433d4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### （最大）池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5807565",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "对二维卷积层输出数据进行池化的池化层也一定是二维的\n",
    "其中输入、输出数据的维度顺序为 (N, C, H, W)\n",
    "池化不会改变输入数据的通道数，只是精炼数据，缩小长与宽（在每个样本的每一个通道对应的特征图上分别进行精炼数据）\n",
    "\n",
    "kernel_size    -- 池化运算的窗口大小\n",
    "stride         -- 窗口移动的步幅\n",
    "padding        -- 对输入数据的填充\n",
    "'''\n",
    "pool = torch.nn.MaxPool2d(kernel_size=2, stride=1, padding=0)\n",
    "\n",
    "# 最大池化是提取窗口内的最大值（在图像处理上用的很多）\n",
    "# 池化层也是没有模型参数的，像激活函数层一样，只是对输入数据做指定的运算操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cbe6f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.4521, -0.0928,  1.8533],\n",
       "          [ 0.3491, -1.3944,  2.1962],\n",
       "          [-0.6643, -0.3087,  0.2284]],\n",
       "\n",
       "         [[ 1.4465, -0.6853,  1.0208],\n",
       "          [-0.9505,  0.5066,  1.2384],\n",
       "          [-0.4284,  0.8077, -0.1590]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7662, -0.7766, -0.5460],\n",
       "          [ 0.6284, -0.0870, -0.6088],\n",
       "          [-1.2213,  0.3833, -0.4907]],\n",
       "\n",
       "         [[ 0.8547,  0.6421, -1.1427],\n",
       "          [-0.7311,  1.4001,  0.5353],\n",
       "          [-0.2117,  0.4510,  0.8516]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 2, 3, 3)   # 2 个大小为 3 * 3 、通道数为 2 的数据\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f5fa4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3491,  2.1962],\n",
       "          [ 0.3491,  2.1962]],\n",
       "\n",
       "         [[ 1.4465,  1.2384],\n",
       "          [ 0.8077,  1.2384]]],\n",
       "\n",
       "\n",
       "        [[[ 0.7662, -0.0870],\n",
       "          [ 0.6284,  0.3833]],\n",
       "\n",
       "         [[ 1.4001,  1.4001],\n",
       "          [ 1.4001,  1.4001]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pool(x)       # 调用池化层的 call 魔法方法来进行正向传播\n",
    "print(output.shape)    # 只调整数据的长宽，通道数和样本数是不变的\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4dbaf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Flatten层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c06ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Flatten 层\n",
    "flatten = torch.nn.Flatten()\n",
    "\n",
    "# 用于将每一个样本对应的数据拉平成一维数组\n",
    "# 常用于对卷积层的输出数据作转换，输入到全连接层中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fcae6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 48])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3, 4, 4)\n",
    "output = flatten(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7a599",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dropout层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4dbf41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "636461a1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### BatchNorm层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c489a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f292d582",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### MSE损失层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eb625d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7880)\n"
     ]
    }
   ],
   "source": [
    "# 创建 MSE 损失层\n",
    "mseloss = torch.nn.MSELoss()\n",
    "\n",
    "# 要求输入的预测数据与真实数据的形状一致\n",
    "# 计算对应位置差的平方和\n",
    "x = torch.randn(3, 4)\n",
    "label = torch.rand(3, 4)\n",
    "\n",
    "# 通过调用损失层的 call 魔法方法进行正向传播\n",
    "loss = mseloss(x, label)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dbcd0d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 交叉熵损失层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f061a21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9005) tensor(0.9005)\n"
     ]
    }
   ],
   "source": [
    "# 创建交叉熵损失层（交叉熵损失在分类问题中使用的较多）\n",
    "crossentropyloss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 预测数据形状为 (N, M)，表示 N 个样本对应是 M 个类别的概率\n",
    "x = torch.tensor([[0.1, 0.2, 0.7],\n",
    "                  [0.3, 0.4, 0.3]], dtype=torch.float32)\n",
    "\n",
    "# 真实数据形状为 (N,)，表示 N 个样本的真实类别是哪一个（编号从 0 到 M - 1）\n",
    "label1 = torch.tensor([2, 1])\n",
    "\n",
    "# 真实数据也可以是预测数据同样的形状，表示各个样本是各种类型的概率\n",
    "label2 = torch.tensor([[0, 0, 1],\n",
    "                       [0, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "# 通过调用损失层的 call 魔法方法进行正向传播\n",
    "loss1 = crossentropyloss(x, label1)\n",
    "loss2 = crossentropyloss(x, label2)\n",
    "print(loss1, loss2)\n",
    "\n",
    "# 注意：这个交叉熵损失层再内部是先对数据应用 Softmax 函数的，故使用时不需要额外在其前面添加 Softmax 层"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
