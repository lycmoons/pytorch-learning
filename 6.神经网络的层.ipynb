{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6da01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45cd135",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 全连接层（线性层）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60fe4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定输入以及输出神经元的数量，创建全连接层\n",
    "linear = torch.nn.Linear(in_features=10, out_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9355f31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[ 1.6204e-01, -2.0444e-01, -1.2897e-02,  1.6852e-01,  2.9327e-01,\n",
       "          -1.0507e-01,  8.5197e-02, -8.1483e-02,  1.0794e-01, -1.1018e-01],\n",
       "         [ 2.7556e-01,  3.5181e-02,  1.7725e-01, -1.5533e-03, -4.9809e-02,\n",
       "           1.8827e-01, -2.3475e-01,  1.0436e-01,  3.4916e-02, -1.4363e-01],\n",
       "         [ 1.5250e-01,  3.1006e-01, -1.0069e-01, -3.7223e-02,  2.8846e-01,\n",
       "           3.1428e-01, -1.3007e-01,  2.4839e-01,  7.2949e-02,  2.2329e-01],\n",
       "         [ 6.8134e-02, -2.6406e-01,  1.3295e-01, -2.2816e-01,  1.9616e-01,\n",
       "          -2.9166e-01,  2.7269e-02,  1.4765e-02, -2.2855e-04, -2.2411e-04],\n",
       "         [ 9.3534e-03, -5.4455e-02, -9.8700e-02, -2.3186e-05, -2.6181e-01,\n",
       "          -2.8278e-02,  6.2345e-02, -2.1410e-01, -7.6263e-02,  2.3914e-01]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1452, -0.2504,  0.1929,  0.2126,  0.2083], requires_grad=True))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建好全连接层后，会自动初始化权重与偏置\n",
    "linear.weight, linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bab0109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# 调用全连接层的正向传播方法（从输入到输出）\n",
    "x = torch.arange(30, dtype=torch.float32).reshape(3, 10)    # 3 个样本，每个样本 10 个输入\n",
    "output = linear(x)                                          # 通过调用 call 魔法方法即可实现正向传播，获取输出\n",
    "print(output.shape)                                         # 分别得到 3 个样本的输出，每个样本对应 5 个输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97b044",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### ReLU层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6083d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接创建 ReLU 层，不需要任何参数\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# 该层也没有任何模型参数，只是单纯的对输入的任何元素施加 ReLU 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc681f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 调用 ReLU 层\n",
    "x = torch.tensor([1, 2, -3, -0.5])\n",
    "output = relu(x)                    # 也是通过调用 call 魔法方法来实现正向传播\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f50b15",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Sigmoid层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd6eb022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7311, 0.8808, 0.0474, 0.3775])\n"
     ]
    }
   ],
   "source": [
    "# 整体使用方法与 ReLU 层类似\n",
    "sigmoid = torch.nn.Sigmoid()          # 创建 Sigmoid 层\n",
    "x = torch.tensor([1, 2, -3, -0.5])    # 测试样本\n",
    "output = sigmoid(x)                   # 通过调用 call 魔法方法实现正向传播\n",
    "print(output)                         # 相当于对每一个输入元素调用 Sigmoid 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d0455",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Softmax层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af243567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定应用 Softmax 函数的维度，并创建 Softmax 层\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "# 一般认为一行表示一个样本，对每一行应用 Softmax 函数（dim = 1）\n",
    "# 即最终各行元素之和均为 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f9f4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5016, 0.1687, 0.1253, 0.0173, 0.1146, 0.0726],\n",
      "        [0.0908, 0.0475, 0.0767, 0.3466, 0.3754, 0.0630],\n",
      "        [0.3311, 0.1088, 0.0659, 0.0915, 0.1738, 0.2288]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 6)    # 3 个样本，6 个输出（在分类任务中表示对 6 个类别的输出）\n",
    "output = softmax(x)      # 在每一行元素上应用 Softmax 函数（得到对应 6 个类别的概率）\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3712b7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sum(dim=1)      # 各行元素之和均为 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95f408",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3552304",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "卷积运算用的最多的是二维卷积（对图像进行处理）\n",
    "其中输入、输出数据的维度顺序为 (N, C, H, W)\n",
    "\n",
    "in_channels    -- 输入数据的通道数，也是核的通道数\n",
    "out_channels   -- 输出数据的通道数，也是核的个数\n",
    "kernel_size    -- 核的大小\n",
    "stride         -- 卷积运算的步幅\n",
    "padding        -- 对输入数据的填充\n",
    "'''\n",
    "\n",
    "# 通过指定卷积层的参数来创建卷积层\n",
    "convolution = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb6479d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[[[ 0.1534,  0.1777, -0.1103],\n",
       "           [ 0.1747,  0.1320, -0.0668],\n",
       "           [ 0.1125,  0.1222,  0.1434]],\n",
       " \n",
       "          [[ 0.0575,  0.1118, -0.1237],\n",
       "           [ 0.1854,  0.1912,  0.0103],\n",
       "           [ 0.0679,  0.0738,  0.1576]],\n",
       " \n",
       "          [[ 0.1380, -0.1378,  0.0348],\n",
       "           [ 0.0352,  0.0284, -0.0085],\n",
       "           [-0.0561, -0.1280, -0.0003]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0097,  0.1286, -0.0472],\n",
       "           [-0.1360,  0.0547,  0.1376],\n",
       "           [-0.1850, -0.1543,  0.1505]],\n",
       " \n",
       "          [[ 0.1173,  0.1756, -0.1196],\n",
       "           [-0.0359, -0.0743, -0.1007],\n",
       "           [-0.0190,  0.0543,  0.1252]],\n",
       " \n",
       "          [[ 0.1004, -0.1186,  0.0975],\n",
       "           [ 0.0920,  0.1349, -0.0147],\n",
       "           [ 0.1074, -0.0073, -0.0664]]],\n",
       " \n",
       " \n",
       "         [[[-0.0838,  0.1028, -0.0808],\n",
       "           [ 0.1042,  0.0259,  0.1161],\n",
       "           [ 0.0499, -0.0709, -0.0811]],\n",
       " \n",
       "          [[-0.0486,  0.1491, -0.0394],\n",
       "           [ 0.0668,  0.1419,  0.1181],\n",
       "           [-0.1490,  0.1301,  0.0951]],\n",
       " \n",
       "          [[-0.1797, -0.1636,  0.0954],\n",
       "           [ 0.0919,  0.1085,  0.0832],\n",
       "           [-0.0826, -0.1195, -0.1362]]],\n",
       " \n",
       " \n",
       "         [[[-0.1805, -0.0471, -0.0740],\n",
       "           [-0.0478,  0.0401, -0.1829],\n",
       "           [ 0.0829,  0.0025,  0.1409]],\n",
       " \n",
       "          [[-0.0768,  0.0043, -0.1860],\n",
       "           [ 0.0355, -0.1059, -0.1870],\n",
       "           [-0.1614,  0.0115,  0.1024]],\n",
       " \n",
       "          [[-0.1575,  0.1761, -0.0397],\n",
       "           [ 0.1280, -0.0839,  0.0309],\n",
       "           [ 0.0748, -0.1750,  0.0097]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1778, -0.1494, -0.0943],\n",
       "           [-0.0589,  0.1714, -0.1534],\n",
       "           [-0.1252, -0.0973,  0.0036]],\n",
       " \n",
       "          [[-0.0954,  0.1802,  0.1549],\n",
       "           [-0.0187,  0.0417, -0.0036],\n",
       "           [ 0.1178, -0.1762,  0.1783]],\n",
       " \n",
       "          [[ 0.1637, -0.1733,  0.0855],\n",
       "           [ 0.0357, -0.0371,  0.1837],\n",
       "           [-0.1611,  0.0507,  0.1268]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0519,  0.1405,  0.1724],\n",
       "           [-0.0506, -0.0422,  0.1046],\n",
       "           [-0.1658,  0.0858, -0.1541]],\n",
       " \n",
       "          [[ 0.1094,  0.0984, -0.1540],\n",
       "           [ 0.0777,  0.1042,  0.1323],\n",
       "           [-0.0895,  0.0940,  0.1432]],\n",
       " \n",
       "          [[ 0.1104, -0.0487,  0.1052],\n",
       "           [ 0.0454, -0.0472,  0.0717],\n",
       "           [-0.1404,  0.1318, -0.1542]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1599, -0.1401,  0.0749,  0.1140, -0.1643, -0.0083],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建好卷积层后，会自动初始化好模型参数（卷积核 和 偏置）\n",
    "convolution.weight, convolution.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8de5940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 32, 32)     # 模拟 1 个大小为 32 * 32 的 3 通道（RGB）彩色图像数据\n",
    "output = convolution(x)           # 通过调用 call 魔法方法进行卷积层的正向传播\n",
    "print(output.shape)               # 可以得到通道数等于核的个数的新数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6433d4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### （最大）池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5807565",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "对二维卷积层输出数据进行池化的池化层也一定是二维的\n",
    "其中输入、输出数据的维度顺序为 (N, C, H, W)\n",
    "池化不会改变输入数据的通道数，只是精炼数据，缩小长与宽（在每个样本的每一个通道对应的特征图上分别进行精炼数据）\n",
    "\n",
    "kernel_size    -- 池化运算的窗口大小\n",
    "stride         -- 窗口移动的步幅\n",
    "padding        -- 对输入数据的填充\n",
    "'''\n",
    "pool = torch.nn.MaxPool2d(kernel_size=2, stride=1, padding=0)\n",
    "\n",
    "# 最大池化是提取窗口内的最大值（在图像处理上用的很多）\n",
    "# 池化层也是没有模型参数的，像激活函数层一样，只是对输入数据做指定的运算操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cbe6f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.8901,  1.4404, -0.1949],\n",
       "          [-0.9262,  2.0766, -0.0273],\n",
       "          [ 1.5320,  0.3519,  0.0425]],\n",
       "\n",
       "         [[-0.1658,  0.3645, -0.6759],\n",
       "          [ 0.0734, -1.3039, -0.4407],\n",
       "          [-2.2038, -0.2539,  0.8688]]],\n",
       "\n",
       "\n",
       "        [[[ 2.1428, -1.0313,  1.2831],\n",
       "          [ 1.4973, -0.2758,  0.3096],\n",
       "          [-1.3119, -0.1408,  0.5046]],\n",
       "\n",
       "         [[ 0.6676,  0.3295, -0.1464],\n",
       "          [-1.2856, -0.8622, -0.3977],\n",
       "          [-0.0124, -1.2191, -1.3990]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 2, 3, 3)   # 2 个大小为 3 * 3 、通道数为 2 的数据\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f5fa4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.0766,  2.0766],\n",
       "          [ 2.0766,  2.0766]],\n",
       "\n",
       "         [[ 0.3645,  0.3645],\n",
       "          [ 0.0734,  0.8688]]],\n",
       "\n",
       "\n",
       "        [[[ 2.1428,  1.2831],\n",
       "          [ 1.4973,  0.5046]],\n",
       "\n",
       "         [[ 0.6676,  0.3295],\n",
       "          [-0.0124, -0.3977]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pool(x)       # 调用池化层的 call 魔法方法来进行正向传播\n",
    "print(output.shape)    # 只调整数据的长宽，通道数和样本数是不变的\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9765dd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 自适应平均池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "327ee5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过指定输出数据的 H * W，内部会自动计算出合适的池化窗口大小和步幅\n",
    "global_avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "# 池化过程跟普通池化一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ae08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2, 2, 2)\n",
    "output = global_avg_pool(x)   # 池化不改变样本数和通道数\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4dbaf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Flatten层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5c06ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Flatten 层\n",
    "flatten = torch.nn.Flatten()\n",
    "\n",
    "# 用于将每一个样本对应的数据拉平成一维数组\n",
    "# 常用于对卷积层的输出数据作转换，输入到全连接层中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fcae6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 48])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3, 4, 4)\n",
    "output = flatten(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7a599",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dropout层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4dbf41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "636461a1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### BatchNorm层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116c489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.BatchNorm1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292d582",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### MSE损失层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eb625d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1662)\n"
     ]
    }
   ],
   "source": [
    "# 创建 MSE 损失层\n",
    "mseloss = torch.nn.MSELoss()\n",
    "\n",
    "# 要求输入的预测数据与真实数据的形状一致\n",
    "# 计算对应位置差的平方和\n",
    "x = torch.randn(3, 4)\n",
    "label = torch.rand(3, 4)\n",
    "\n",
    "# 通过调用损失层的 call 魔法方法进行正向传播\n",
    "loss = mseloss(x, label)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dbcd0d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 交叉熵损失层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f061a21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9005) tensor(0.9005)\n"
     ]
    }
   ],
   "source": [
    "# 创建交叉熵损失层（交叉熵损失在分类问题中使用的较多）\n",
    "crossentropyloss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 预测数据形状为 (N, M)，表示 N 个样本对应是 M 个类别的概率\n",
    "x = torch.tensor([[0.1, 0.2, 0.7],\n",
    "                  [0.3, 0.4, 0.3]], dtype=torch.float32)\n",
    "\n",
    "# 真实数据形状为 (N,)，表示 N 个样本的真实类别是哪一个（编号从 0 到 M - 1）\n",
    "label1 = torch.tensor([2, 1])\n",
    "\n",
    "# 真实数据也可以是预测数据同样的形状，表示各个样本是各种类型的概率\n",
    "label2 = torch.tensor([[0, 0, 1],\n",
    "                       [0, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "# 通过调用损失层的 call 魔法方法进行正向传播\n",
    "loss1 = crossentropyloss(x, label1)\n",
    "loss2 = crossentropyloss(x, label2)\n",
    "print(loss1, loss2)\n",
    "\n",
    "# 注意：这个交叉熵损失层再内部是先对数据应用 Softmax 函数的，故使用时不需要额外在其前面添加 Softmax 层"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
