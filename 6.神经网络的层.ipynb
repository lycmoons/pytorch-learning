{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f6da01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45cd135",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 全连接层（线性层）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60fe4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定输入以及输出神经元的数量，创建全连接层\n",
    "linear = torch.nn.Linear(in_features=10, out_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9355f31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-0.2355,  0.2675, -0.2939,  0.2603, -0.0859, -0.1308, -0.0644, -0.0422,\n",
       "          -0.1558,  0.1965],\n",
       "         [-0.1766,  0.0329, -0.2180, -0.0748, -0.0264,  0.0452, -0.0379,  0.0619,\n",
       "           0.0035,  0.2743],\n",
       "         [ 0.1485,  0.2862, -0.2397, -0.0540, -0.0871,  0.1411,  0.1568, -0.1105,\n",
       "           0.2351,  0.0308],\n",
       "         [-0.0897, -0.1097, -0.2668, -0.0086,  0.1994, -0.0386, -0.0208,  0.1984,\n",
       "          -0.2557, -0.1468],\n",
       "         [ 0.1369,  0.3115,  0.2172,  0.1685, -0.0550,  0.1403,  0.2265,  0.0745,\n",
       "          -0.2470,  0.2531]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.2688, -0.0651,  0.2951,  0.2930,  0.2708], requires_grad=True))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建好全连接层后，会自动初始化权重与偏置\n",
    "linear.weight, linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bab0109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# 调用全连接层的正向传播方法（从输入到输出）\n",
    "x = torch.arange(30, dtype=torch.float32).reshape(3, 10)    # 3 个样本，每个样本 10 个输入\n",
    "output = linear(x)                                          # 通过调用 call 魔法方法即可实现正向传播，获取输出\n",
    "print(output.shape)                                         # 分别得到 3 个样本的输出，每个样本对应 5 个输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97b044",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### ReLU层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b6083d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接创建 ReLU 层，不需要任何参数\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "# 该层也没有任何模型参数，只是单纯的对输入的任何元素施加 ReLU 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc681f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 调用 ReLU 层\n",
    "x = torch.tensor([1, 2, -3, -0.5])\n",
    "output = relu(x)                    # 也是通过调用 call 魔法方法来实现正向传播\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f50b15",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Sigmoid层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd6eb022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7311, 0.8808, 0.0474, 0.3775])\n"
     ]
    }
   ],
   "source": [
    "# 整体使用方法与 ReLU 层类似\n",
    "sigmoid = torch.nn.Sigmoid()          # 创建 Sigmoid 层\n",
    "x = torch.tensor([1, 2, -3, -0.5])    # 测试样本\n",
    "output = sigmoid(x)                   # 通过调用 call 魔法方法实现正向传播\n",
    "print(output)                         # 相当于对每一个输入元素调用 Sigmoid 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d0455",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Softmax层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af243567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定应用 Softmax 函数的维度，并创建 Softmax 层\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "# 一般认为一行表示一个样本，对每一行应用 Softmax 函数（dim = 1）\n",
    "# 即最终各行元素之和均为 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64f9f4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2039, 0.0245, 0.4770, 0.1043, 0.1092, 0.0811],\n",
      "        [0.1341, 0.1709, 0.4744, 0.0226, 0.0254, 0.1726],\n",
      "        [0.2263, 0.2155, 0.3269, 0.0577, 0.1165, 0.0571]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 6)    # 3 个样本，6 个输出（在分类任务中表示对 6 个类别的输出）\n",
    "output = softmax(x)      # 在每一行元素上应用 Softmax 函数（得到对应 6 个类别的概率）\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3712b7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sum(dim=1)      # 各行元素之和均为 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95f408",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3552304",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "卷积运算用的最多的是二维卷积（对图像进行处理）\n",
    "其中输入、输出数据的维度顺序为 (N, C, H, W)\n",
    "\n",
    "in_channels    -- 输入数据的通道数，也是核的通道数\n",
    "out_channels   -- 输出数据的通道数，也是核的个数\n",
    "kernel_size    -- 核的大小\n",
    "stride         -- 卷积运算的步幅\n",
    "padding        -- 对输入数据的填充\n",
    "'''\n",
    "\n",
    "# 通过指定卷积层的参数来创建卷积层\n",
    "convolution = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bb6479d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[[[ 0.1351, -0.1173,  0.0559],\n",
       "           [ 0.0990,  0.0574,  0.0680],\n",
       "           [-0.0063, -0.0274,  0.0987]],\n",
       " \n",
       "          [[ 0.1488, -0.0757,  0.0924],\n",
       "           [ 0.1281, -0.0803, -0.0116],\n",
       "           [ 0.0384,  0.1266, -0.1306]],\n",
       " \n",
       "          [[-0.0797, -0.0502,  0.0239],\n",
       "           [ 0.1057,  0.0533, -0.1075],\n",
       "           [-0.0707, -0.0815,  0.0532]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1221, -0.1029,  0.0693],\n",
       "           [ 0.0113, -0.1001, -0.0091],\n",
       "           [ 0.0993, -0.0327, -0.0429]],\n",
       " \n",
       "          [[ 0.0732, -0.1477,  0.1337],\n",
       "           [ 0.0099, -0.1304, -0.1820],\n",
       "           [-0.1756,  0.0518, -0.1146]],\n",
       " \n",
       "          [[ 0.0733,  0.0408,  0.0644],\n",
       "           [-0.0865,  0.1839, -0.0496],\n",
       "           [-0.1213, -0.0256, -0.0917]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1539, -0.1328, -0.0516],\n",
       "           [-0.1097, -0.1562, -0.0303],\n",
       "           [ 0.0979,  0.1096,  0.1317]],\n",
       " \n",
       "          [[-0.0895,  0.0556,  0.1214],\n",
       "           [-0.1729, -0.1597,  0.1682],\n",
       "           [ 0.0849, -0.0089, -0.0941]],\n",
       " \n",
       "          [[ 0.0509,  0.0062,  0.0303],\n",
       "           [-0.0811,  0.1219, -0.0761],\n",
       "           [ 0.0759, -0.1855,  0.1417]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1741, -0.1588,  0.1260],\n",
       "           [-0.0549, -0.1071,  0.0833],\n",
       "           [ 0.0317,  0.0015,  0.0794]],\n",
       " \n",
       "          [[-0.0555,  0.1156, -0.1415],\n",
       "           [ 0.0604, -0.0337, -0.1862],\n",
       "           [-0.0522,  0.1478,  0.0672]],\n",
       " \n",
       "          [[ 0.1107,  0.1767,  0.0542],\n",
       "           [ 0.0131, -0.1837,  0.0859],\n",
       "           [ 0.0260, -0.0908,  0.1333]]],\n",
       " \n",
       " \n",
       "         [[[-0.0042,  0.1303,  0.0656],\n",
       "           [ 0.1749, -0.1091, -0.0851],\n",
       "           [ 0.0582, -0.1654,  0.1515]],\n",
       " \n",
       "          [[ 0.1201, -0.1427, -0.1028],\n",
       "           [-0.0566,  0.1220, -0.0715],\n",
       "           [-0.0228, -0.0738, -0.0080]],\n",
       " \n",
       "          [[ 0.1108, -0.1734,  0.0226],\n",
       "           [ 0.0313, -0.1330,  0.0958],\n",
       "           [-0.1487,  0.0020, -0.0765]]],\n",
       " \n",
       " \n",
       "         [[[-0.0607, -0.0885,  0.0027],\n",
       "           [-0.0502,  0.0014, -0.0215],\n",
       "           [ 0.0009, -0.1326, -0.0704]],\n",
       " \n",
       "          [[-0.0507,  0.0593, -0.0578],\n",
       "           [-0.1525,  0.1127,  0.1586],\n",
       "           [ 0.1623,  0.0971,  0.0699]],\n",
       " \n",
       "          [[-0.0177, -0.0948,  0.1865],\n",
       "           [-0.0480, -0.1146, -0.1431],\n",
       "           [-0.1132, -0.1229,  0.0487]]]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1876, -0.0077, -0.0170, -0.0379,  0.0766,  0.0207],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建好卷积层后，会自动初始化好模型参数（卷积核 和 偏置）\n",
    "convolution.weight, convolution.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8de5940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 32, 32)     # 模拟 1 个大小为 32 * 32 的 3 通道（RGB）彩色图像数据\n",
    "output = convolution(x)           # 通过调用 call 魔法方法进行卷积层的正向传播\n",
    "print(output.shape)               # 可以得到通道数等于核的个数的新数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6433d4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### （最大）池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5807565",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "对二维卷积层输出数据进行池化的池化层也一定是二维的\n",
    "其中输入、输出数据的维度顺序为 (N, C, H, W)\n",
    "池化不会改变输入数据的通道数，只是精炼数据，缩小长与宽（在每个样本的每一个通道对应的特征图上分别进行精炼数据）\n",
    "\n",
    "kernel_size    -- 池化运算的窗口大小\n",
    "stride         -- 窗口移动的步幅\n",
    "padding        -- 对输入数据的填充\n",
    "'''\n",
    "pool = torch.nn.MaxPool2d(kernel_size=2, stride=1, padding=0)\n",
    "\n",
    "# 最大池化是提取窗口内的最大值（在图像处理上用的很多）\n",
    "# 池化层也是没有模型参数的，像激活函数层一样，只是对输入数据做指定的运算操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cbe6f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5825, -0.5555, -0.1655],\n",
       "          [ 2.1773,  0.5176,  0.2473],\n",
       "          [-2.0066,  0.7154,  1.1815]],\n",
       "\n",
       "         [[-0.1344,  0.7696, -0.1264],\n",
       "          [-0.8483,  0.6956, -0.3064],\n",
       "          [-0.7610,  0.2390, -0.0809]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4137, -1.1263,  0.9501],\n",
       "          [-0.5783,  0.0638,  0.7853],\n",
       "          [ 1.0699, -1.7613, -0.0602]],\n",
       "\n",
       "         [[-0.5682, -1.0803, -0.2424],\n",
       "          [-0.6876,  1.2280, -1.7699],\n",
       "          [ 1.1600,  0.5254, -0.0061]]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 2, 3, 3)   # 2 个大小为 3 * 3 、通道数为 2 的数据\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f5fa4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[2.1773, 0.5176],\n",
       "          [2.1773, 1.1815]],\n",
       "\n",
       "         [[0.7696, 0.7696],\n",
       "          [0.6956, 0.6956]]],\n",
       "\n",
       "\n",
       "        [[[0.4137, 0.9501],\n",
       "          [1.0699, 0.7853]],\n",
       "\n",
       "         [[1.2280, 1.2280],\n",
       "          [1.2280, 1.2280]]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pool(x)       # 调用池化层的 call 魔法方法来进行正向传播\n",
    "print(output.shape)    # 只调整数据的长宽，通道数和样本数是不变的\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9765dd",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 自适应平均池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "327ee5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过指定输出数据的 H * W，内部会自动计算出合适的池化窗口大小和步幅\n",
    "global_avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "# 池化过程跟普通池化一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ae0ae08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2, 2, 2)\n",
    "output = global_avg_pool(x)   # 池化不改变样本数和通道数\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4dbaf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Flatten层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5c06ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Flatten 层\n",
    "flatten = torch.nn.Flatten()\n",
    "\n",
    "# 用于将每一个样本对应的数据拉平成一维数组\n",
    "# 常用于对卷积层的输出数据作转换，输入到全连接层中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3fcae6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 48])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3, 4, 4)\n",
    "output = flatten(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7a599",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dropout层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f4dbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以接受任意形状的输入，按照 p 概率对每一个位置的数据进行置零\n",
    "# 然后对于保留下来的数据进行拉伸（除 1 - dropout），保持数据的期望\n",
    "dropout = torch.nn.Dropout(p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "747591bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1443, -2.0384],\n",
      "         [ 1.0388, -0.6286]],\n",
      "\n",
      "        [[ 0.9508, -0.5222],\n",
      "         [ 0.1518, -0.8859]]])\n",
      "tensor([[[ 0.2887, -0.0000],\n",
      "         [ 2.0777, -1.2572]],\n",
      "\n",
      "        [[ 1.9016, -0.0000],\n",
      "         [ 0.0000, -1.7718]]])\n"
     ]
    }
   ],
   "source": [
    "dropout.train()\n",
    "x = torch.randn(2, 2, 2)\n",
    "print(x)\n",
    "print(dropout(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ffae18ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1443, -2.0384],\n",
      "         [ 1.0388, -0.6286]],\n",
      "\n",
      "        [[ 0.9508, -0.5222],\n",
      "         [ 0.1518, -0.8859]]])\n",
      "tensor([[[ 0.1443, -2.0384],\n",
      "         [ 1.0388, -0.6286]],\n",
      "\n",
      "        [[ 0.9508, -0.5222],\n",
      "         [ 0.1518, -0.8859]]])\n"
     ]
    }
   ],
   "source": [
    "# 在训练时 Dropout 层处理如上\n",
    "# 但在测试时，Dropout 层不做任何处理\n",
    "dropout.eval()\n",
    "print(x)\n",
    "print(dropout(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636461a1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### BatchNorm层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "116c489a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.batchnorm.BatchNorm1d"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.BatchNorm1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7108cb8e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### LayerNorm层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "090d4876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  1.,  2.],\n",
      "          [ 3.,  4.,  5.]],\n",
      "\n",
      "         [[ 6.,  7.,  8.],\n",
      "          [ 9., 10., 11.]]],\n",
      "\n",
      "\n",
      "        [[[12., 13., 14.],\n",
      "          [15., 16., 17.]],\n",
      "\n",
      "         [[18., 19., 20.],\n",
      "          [21., 22., 23.]]]])\n",
      "tensor([[[[-1.2247,  0.0000,  1.2247],\n",
      "          [-1.2247,  0.0000,  1.2247]],\n",
      "\n",
      "         [[-1.2247,  0.0000,  1.2247],\n",
      "          [-1.2247,  0.0000,  1.2247]]],\n",
      "\n",
      "\n",
      "        [[[-1.2247,  0.0000,  1.2247],\n",
      "          [-1.2247,  0.0000,  1.2247]],\n",
      "\n",
      "         [[-1.2247,  0.0000,  1.2247],\n",
      "          [-1.2247,  0.0000,  1.2247]]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "norm = torch.nn.LayerNorm(3)\n",
    "x = torch.arange(24).reshape(2, 2, 2, 3).type(torch.float32)\n",
    "print(x)\n",
    "print(norm(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292d582",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### MSE损失层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2eb625d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3262)\n"
     ]
    }
   ],
   "source": [
    "# 创建 MSE 损失层\n",
    "mseloss = torch.nn.MSELoss()\n",
    "\n",
    "# 要求输入的预测数据与真实数据的形状一致\n",
    "# 计算对应位置差的平方和\n",
    "x = torch.randn(3, 4)\n",
    "label = torch.rand(3, 4)\n",
    "\n",
    "# 通过调用损失层的 call 魔法方法进行正向传播\n",
    "loss = mseloss(x, label)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dbcd0d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 交叉熵损失层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f061a21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9005) tensor(0.9005)\n"
     ]
    }
   ],
   "source": [
    "# 创建交叉熵损失层（交叉熵损失在分类问题中使用的较多）\n",
    "crossentropyloss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 预测数据形状为 (N, M)，表示 N 个样本对应是 M 个类别的概率\n",
    "x = torch.tensor([[0.1, 0.2, 0.7],\n",
    "                  [0.3, 0.4, 0.3]], dtype=torch.float32)\n",
    "\n",
    "# 真实数据形状为 (N,)，表示 N 个样本的真实类别是哪一个（编号从 0 到 M - 1）\n",
    "label1 = torch.tensor([2, 1])\n",
    "\n",
    "# 真实数据也可以是预测数据同样的形状，表示各个样本是各种类型的概率\n",
    "label2 = torch.tensor([[0, 0, 1],\n",
    "                       [0, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "# 通过调用损失层的 call 魔法方法进行正向传播\n",
    "loss1 = crossentropyloss(x, label1)\n",
    "loss2 = crossentropyloss(x, label2)\n",
    "print(loss1, loss2)\n",
    "\n",
    "# 注意：这个交叉熵损失层再内部是先对数据应用 Softmax 函数的，故使用时不需要额外在其前面添加 Softmax 层"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
