{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b2a607",
   "metadata": {},
   "source": [
    "### LSTM总览\n",
    "\n",
    "于 GRU 一样，LSTM 也是在 RNN 的基础上，希望控制历史信息的保留程度而设计出来的\n",
    "\n",
    "相比 GRU，LSTM 有更多的中间值\n",
    "\n",
    "同时也多一个潜变量（需要使用历史值进行更新的变量），也就是记忆细胞状态\n",
    "\n",
    "遗忘门（Forget Gate），决定保留多少旧的细胞状态：\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)\n",
    "$$\n",
    "\n",
    "输入门（Input Gate），控制当前输入能带入多少新信息：\n",
    "\n",
    "$$\n",
    "i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)\n",
    "$$\n",
    "\n",
    "输出门（Output Gate），决定输出多少当前状态：\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)\n",
    "$$\n",
    "\n",
    "候选细胞状态（Candidate Cell State），生成候选的新记忆：\n",
    "\n",
    "$$\n",
    "\\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c)\n",
    "$$\n",
    "\n",
    "更新细胞状态，组合旧记忆和新记忆：\n",
    "\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "$$\n",
    "\n",
    "最终隐藏状态（Hidden State），由当前细胞状态和输出门共同决定：\n",
    "\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh(c_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983137ae",
   "metadata": {},
   "source": [
    "使用 LSTM 预测一个 token 的过程图解如下：\n",
    "\n",
    "![](md-img/LSTM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa274eff",
   "metadata": {},
   "source": [
    "$$\n",
    "其中 C 和 H 都是潜变量，且 H、F、I、O、C、\\tilde{C} 的形状一致\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d7085",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 代码从零实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fcf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LSTMModel:\n",
    "    # 保存模型参数\n",
    "    def __init__(self, vocab_size, hiden_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hiden_size = hiden_size\n",
    "\n",
    "        self.w_hf = torch.randn(hiden_size, hiden_size) * 0.01\n",
    "        self.w_xf = torch.randn(vocab_size, hiden_size) * 0.01\n",
    "        self.b_f = torch.zeros(hiden_size)\n",
    "        self.w_hi = torch.randn(hiden_size, hiden_size) * 0.01\n",
    "        self.w_xi = torch.randn(vocab_size, hiden_size) * 0.01\n",
    "        self.b_i = torch.zeros(hiden_size)\n",
    "        self.w_ho = torch.randn(hiden_size, hiden_size) * 0.01\n",
    "        self.w_xo = torch.randn(vocab_size, hiden_size) * 0.01\n",
    "        self.b_o = torch.zeros(hiden_size)\n",
    "        self.w_hc = torch.randn(hiden_size, hiden_size) * 0.01\n",
    "        self.w_xc = torch.randn(vocab_size, hiden_size) * 0.01\n",
    "        self.b_c = torch.zeros(hiden_size)\n",
    "        self.w_hy = torch.randn(hiden_size, vocab_size) * 0.01\n",
    "        self.b_y = torch.zeros(vocab_size)\n",
    "\n",
    "        self.parameters = [self.w_hf, self.w_xf, self.b_f,\n",
    "                           self.w_hi, self.w_xi, self.b_i,\n",
    "                           self.w_ho, self.w_xo, self.b_o,\n",
    "                           self.w_hc, self.w_xc, self.b_c,\n",
    "                           self.w_hy, self.b_y]\n",
    "        \n",
    "        for param in self.parameters:\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "    # 正向传播\n",
    "    # 输入数据形状为 (time_step, batch_size, vocab_size)\n",
    "    def forward(self, X):\n",
    "        h = torch.zeros(X.shape[1], self.hiden_size)    # 初始化隐藏状态\n",
    "        c = torch.zeros(X.shape[1], self.hiden_size)    # 初始化记忆细胞\n",
    "\n",
    "        Y = []   # 用于保存所有的预测输出\n",
    "\n",
    "        # 按找时间步长，往后推算每一个样本的潜变量（隐藏状态、记忆细胞）\n",
    "        for x in X:\n",
    "            f = torch.sigmoid(h @ self.w_hf + x @ self.w_xf + self.b_f)    # 遗忘门\n",
    "            i = torch.sigmoid(h @ self.w_hi + x @ self.w_xi + self.b_i)    # 输入门\n",
    "            o = torch.sigmoid(h @ self.w_ho + x @ self.w_xo + self.b_o)    # 输出门\n",
    "            c_ = torch.tanh(h @ self.w_hc + x @ self.w_xc + self.b_c)   # 候选记忆细胞\n",
    "\n",
    "            c = f * c + i * c_       # 更新记忆细胞\n",
    "            h = o * torch.tanh(c)    # 更新隐藏状态\n",
    "\n",
    "            # 使用隐藏状态获取最终输出\n",
    "            output = h @ self.w_hy + self.b_y\n",
    "            Y.append(output)\n",
    "        \n",
    "        # 此处返回的 Y 的形状为 (time_step * batch_size, vocab_size)，方便计算损失函数\n",
    "        return torch.cat(Y, dim=0), (h, c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
